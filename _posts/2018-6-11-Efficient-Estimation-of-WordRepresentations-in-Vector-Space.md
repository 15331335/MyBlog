---
layout: post
title: "论文笔记 1"
description: "该笔记所阅读的论文为 Tomas Mikolov 的 Efficient Estimation of WordRepresentations in Vector Space 。"
thumb_image: "documentation/sample-image.jpg"
tags: [reading]
---

## 论文概述

该论文首先介绍了两个基本的神经网络语言模型：

1. **Feedforward Neural Net Language Model (NNLM)**
2. **Recurrent Neural Net Language Model (RNNLM)**

为了最小化从大数据中计算**词分布式表示**的复杂度，本文提出了两个新的对数线性模型：

1. **Continuous Bag-of-Words Model**
2. **Continuous Skip-gram Model**

并且，为了衡量模型词表征结果的质量，本文设计了一个综合的测试集用于测量词向量在**语法和语义**上的相似性。

通过比较多个不同技术和模型在不同的词维度和训练数据量下的测试表现，本文表示**新的模型不仅大幅降低了计算成本，而且显著提升了测试的精确度**。

下面根据上述的内容进一步地介绍该论文。



### 论文目标

在阅读论文时，首先明确论文所处的研究领域及针对的基本问题：

- 该论文主要涉及**自然语言处理 (NPL)** 和基于**神经网络**的机器学习技术。
- 其讨论的问题是：相对复杂的模型如何**高效地**在海量数据中学习得到高质量的词向量。

从之前的许多研究可以发现：具有简单性、健壮性，并且基于大数据训练的简单模型优于基于少量数据训练的复杂系统。可以理解的是，增大训练的数据量可以很好的改善模型本身简单带来的问题。

那能否在海量数据上训练复杂模型呢？根据目前的机器学习技术而言，这是可行的。显然，随着训练数据量的增多，计算的复杂度显著增加，一个很迫切的问题就是如何降低在海量数据训练上的计算复杂度。针对这个问题，而本文就提出了其解决办法。

在诸多论文的研究中，可以发现一些有趣的现象：

- 在测量词向量上，相似的词往往彼此相邻，并且具有多个相似度数。

- 单词表示的相似性超过了简单的语法规则
- 利用词偏移技术（在词向量上实现简单代数运算），发现 `vector(”King”) - vector(”Man”) + vector(”Woman”) ` 的结果最接近 `Queen`。

本文认为词之间的线性规律所值得保留，并且把这当作衡量模型优劣的重要部分。所以开发新的模型旨在最大化这些向量操作的准确度。

结合上述的基本目标和研究现象，本文所做的工作还包括：

- 设计新的综合测试集来计算语法和语义规律，并可被学习到高精确度。
- 讨论训练时间和准确度如何取决于单词向量到维度和训练数据的数量。
- 实现神经网络的并行训练。



### 论文模型

**首先定义的是模型计算复杂度的基本公式：$O=E\*T\*Q$**

$E$ 是训练周期数，$T$ 是训练集中的词数，$Q$是由模型本身决定。

在下面关于 $Q$ 的描述中：$N$ 为词数，$D$ 为维度数，$H$ 为隐藏层大小，$V$ 是词汇表大小。所有模型使用**随机梯度下降和反向传播**进行训练，同时注意到该论文对于模型的阐述相对较为简单。



#### Feedforward Neural Net Language Model (NNLM)

{% mermaid %}
graph LR;
    A[Input]-->B[Projection];
    B-->C[Hidden];
    C-->D[Output];
{% endmermaid %}

其中复杂度为：$Q=N\*D+N\*D\*H+H\*V$

- 从输入到投影层采用共享一个投影矩阵方法，这部分计算 $N\*D$ 是相对简单的。

- $N$ 所代表的之前的词数，使用 $1$ 到 $V$ 编码。
- 针对词汇表的频率可以使用词汇的 Huffman 二叉树表示，从而减少输出层单元变为$log_2(V)$ ，采用分层的 Softmax 可以只需要 $log_2(Unigram\_perplexity(V))$ 。
- 其中最复杂的部分集中在隐层部分的 $N*D*H$

#### Recurrent Neural Net Language Model (RNNLM)

{% mermaid %}
graph LR;
    A[Input]-->B[Hidden];
    B-->B;
    B-->C[Output];
{% endmermaid %}

其中复杂度为：$Q=H\*H+H\*V$

- 这里去掉了投影层，其中隐层是用循环矩阵 $(D=H)$ 进行延时连接。
- 中间使用短期内存存储过去的信息，可以 `之前的隐层状态 + 当前输入 = 更新的隐层状态` 。
- 同样的最后输出可以采用分层的 Softmax 使得 $H\*V$ 变为 $H\*log_2(V)$ 。
- 因此复杂计算在于 $H\*H$ 。

#### New Log-linear Model

1. **Continuous Bag-of-Words Model**

{% mermaid %}
graph LR;
   w1[W_t-2] --> S[Sum];
   w2[W_t-1] --> S[Sum];
   w3[W_t+1] --> S[Sum];
   w4[W_t+2] --> S[Sum];
   S[Sum] --> w[W_t];
{% endmermaid %}

   其中复杂度为：$Q=N\*D+D\*log_2(V)$

   - 由之前的模型可以知道，**计算的复杂度很大程度上是来自于非线性的隐层**。
   - 这里去掉隐层，而中间的投影层对于所有词是共享的。

2. **Continuous Skip-gram Model**

{% mermaid %}
graph LR;
   w[W_t] --> projection;
   projection --> w1[W_t-2];
   projection --> w2[W_t-1];
   projection --> w3[W_t+1];
   projection --> w4[W_t+2];
{% endmermaid %}

   其中复杂度为：$Q=C\*(D+D\*log_2(V))$

   - $C$ 是上下文词间的最大距离。

- 不同与此前的 CBOW 模型，该模型是在某个词的基础上对上下文范围内的词进行最大程度的分类。（其中涉及到一个对数线性分类器）
- 本文发现增加范围能提高结果词向量的质量。



## 个人感悟和改进方案

由于初次接触自然语言处理（NLP）相关的知识，所以许多概念和知识都得从零开始，下面结合拓展学习相关知识的历程来谈个人感悟，并加入了一些个人思考的改进方案或猜想。（由于并未实现过相关的神经网络应用，对于该论文中设计的神经网络模型就不做深入讨论）

在语言模型上，首当其冲的问题就是词单元该用什么样的数据结构来表示和描述。我在该论文中也可以看到一个反复出现的概念，词向量或词的分布式表示。

我开始尝试更深入地了解词的表示。首先在NLP中最直观和常用的是 **One-hot Representation** ，即每个词表示为一个词表大小的向量，只有一个维度的值为 1 代表了当前的词，其余为 0 。

这样的表示方式优缺点很明显，许多研究都有阐述，这里不在赘述，重要一点就是这样的表示使得任意两个词之间都是孤立的。这也是该论文尝试用高质量的 **Distributional Representation** 改进的一点，从其设计的综合测试集兼顾语义和语法的关系上可见一斑。个人认为，这种思维的改变是非常重要，即发现问题对象间的关联性质。

我设想一种更广泛的关联，即**不同事物间的关联**。在开始查阅资料时发现一种说法：语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语音和图像属于较为底层的原始输入信号，所以后两者更适合做深度学习来学习特征。

我开始思考语言这种高度抽象的事物本质。对于语言而言，是先有语音还是先有文字？文字是否是否只是一种高级的编码？如果是的话，那它的编码对象是什么？在上面的说法中，语言指代的是词、句子、篇章等，也就是单一文字和有结构的文字集合。这里由于汉语习惯，我把词等同于文字。

文字的种类繁多，我在看到语音和图像时，想到的文字或许就是拟声词和象形文字。而文字对于事物的高度抽象，在于它不满足于描述我们的所听（声音）和所见（画面），它开始描述抽象事物，如国家、情感等等。

从上述的关联中可以尝试通过改变文字性质来对其表征，比如象形文字在文字本身书面图像结构上与相应的现实图像做特征关联，从而实现象形文字的语义翻译，而在一些拟声的词汇上，可以通过关联相应的语音来实现特征表示。

从语音上来看，更进一步的假设是：把一篇文章**转换成连续语音**来做上下文分析，其中存在一定的结构特征，因为结合人类的阅读和书写习惯（朗读呼吸间隙、思维理解能力等因素）对于冗长语句可能会呈现一定的规避处理特征。这是个讨巧的个人想法，目的也是试图确立事物间的关联并将抽象事物降维处理，但脱离的本文讨论的研究领域也缺乏实证。

回到之前的语言模型的词表示上，如果用传统的稀疏表示法表示词，在构建语言模型是容易造成维数灾难。同时从实践上看，高维的特征如果要套用深度学习，其复杂度几乎是难以接受的，因此低维的词向量在这里也饱受追捧。这一点从该论文中也是同样的，尽管高维数据的复杂模型在直觉上更能全面描述问题对象，但计算复杂度确实是令人难以接受的，而论文也是试图寻求高效地计算方式使得两者兼得。

论文中描述的还有一点有趣的现象非常值得期待，相似词的词向量距离相近，同时其设计的一种类比的衡量任务对于描述词义间相同性质的关联来说简单而且有效。也就是两个词向量之间的关系，可以直接从这两个向量的差里体现出来。向量的差就是数学上的定义，直接逐位相减。比如 `C(king)−C(queen)≈C(man)−C(woman)` 。更强大的是，与 ` C(king)−C(man)+C(woman) ` 最接近的向量就是 `C(queen)` 。

从语言学的角度来评估词向量，相比于语义上的关系，语法上的关系个人认为价值会低一点。语法上的构词法可作为一定的参考，由于本身就是一种人为的编码规则，可能对于事物特征的表述不明确，用于词性的分类或许更有价值。可能这是由于中文文法与英文文法的差异造成的误解，但只要抓住**不同词单元本身所描述的对象是否相关**这一点来考量都是值得实验的。



_If you find any bugs about this blog please contact me!_
